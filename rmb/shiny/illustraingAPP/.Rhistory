# create a training and testing set from this DF
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confusion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
## do similar steps with PCA
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
ggplot(data=training, aes(x=index, y=CompressiveStrength)) + geom_point()
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
library(ggplot2)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
dim(training);dim(testing)
head(training)
xnames <- colnames(concrete)[1:8]
featurePlot(x=training[, xnames], y=training$CompressiveStrength, plot="pairs")
index <- seq_along(1:nrow(training))
ggplot(data=training, aes(x=index, y=CompressiveStrength)) + geom_point(
ggplot(data=training, aes(x=index, y=CompressiveStrength)) + geom_point()
+
theme_bw()
ggplot(data=training, aes(x=index, y=CompressiveStrength)) + geom_point() +
theme_bw()
index <- seq_along(1:nrow(training))
ggplot(data=training, aes(x=index, y=CompressiveStrength)) + geom_point()
+
theme_bw()
ggplot(data=training, aes(y=index, x=cutCompressiveStrength))
+
geom_boxplot()
library(Hmisc)
cutCompressiveStrength <- cut2(training$CompressiveStrength, g=4)
summary(cutCompressiveStrength)
ggplot(data=training, aes(y=index, x=cutCompressiveStrength)) +
geom_boxplot()
geom_jitter(col="blue")
+ theme_bw()
ggplot(data=training, aes(y=index, x=cutCompressiveStrength)) +
geom_boxplot() + geom_jitter(col="blue")
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
View(iris)
library(caret)
inTrain <- createDataPartition(y=iris$Species, p=0.7,list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training);dim(testing)
View(training)
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)
modFit
?train
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.5)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.5)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("rattle")
library(rattle)
fancyRpartPlot(modFit$finalModel)
preidct(modFit, newdata=testing)
predict(modFit, newdata=testing)
library(ElemStatLearn); data(ozone,package="ElemStatLearn")
ozone
ozone <- ozone[order(ozone$ozone),]
head(ozone)
ll <- matrix(NA,nrow=10,ncol=155)
ll
for(i in 1:10){
ss <- sample(1:dim(ozone)[1],replace=T)
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone,data=ozone0,span=0.2)
ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
}
plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
predictors
temperature
ctreeBag
treebag <- bag(predictors, temperature, B = 10,
bagControl = bagControl(fit = ctreeBag$fit,
predict = ctreeBag$pred,
aggregate = ctreeBag$aggregate))
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B = 10,
bagControl = bagControl(fit = ctreeBag$fit,
predict = ctreeBag$pred,
aggregate = ctreeBag$aggregate))
plot(ozone$ozone,temperature,col='lightgrey',pch=19)
points(ozone$ozone,predict(treebag$fits[[1]]$fit,predictors),pch=19,col="red")
points(ozone$ozone,predict(treebag,predictors),pch=19,col="blue")
library(ElemStatLearn); data(ozone,package="ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
head(ozone)
ll <- matrix(NA,nrow=10,ncol=155)
for(i in 1:10){
ss <- sample(1:dim(ozone)[1],replace=T)
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone,data=ozone0,span=0.2)
ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
}
plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B = 10,
bagControl = bagControl(fit = ctreeBag$fit,
predict = ctreeBag$pred,
aggregate = ctreeBag$aggregate))
plot(ozone$ozone,temperature,col='lightgrey',pch=19)
points(ozone$ozone,predict(treebag$fits[[1]]$fit,predictors),pch=19,col="red")
points(ozone$ozone,predict(treebag,predictors),pch=19,col="blue")
ctreeBag$fit
ctreeBag$pred
ctreeBag$aggregate
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage
View(Wage)
name(Wage)
names(Wage)
Wage <- subset(Wage,select=-c(logwage))
names(Wage)
inTrain <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training)
dim(testing)
?train
training <- Wage[inTrain,];
testing <- Wage[-inTrain,]
?train
qplot(predict(modFit,testing),wage,data=testing)
modFit <- train(wage ~ ., method="gbm",data=training,verbose=FALSE)
qplot(predict(modFit,testing),wage,data=testing)
predict(modFit,testing)
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
modFit <- train(Species~.,data=training,method="rf")
modFit
modFit$Finalmodel
modFit$finalModel
getTree(modFit$finalModel)
getTree(modFit$finalModel,k=2)
?getTree
modFit$finalModel$prox
training[,c(3,4)]
?classCenter
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
pred <- predict(modFit,data = testing)
testing$PredictRight <- pred == testing$Species
pred <- predict(modFit,data = testing)
testing$PredictRight <- pred == testing$Species
pred <- predict(modFit,data = testing)
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
library(caret)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
modFit <- train(Species~.,data=training,method="rf")
modFit
pred <- predict(modFit,data = testing)
pred <- predict(modFit,testing)
testing$PredictRight <- pred == testing$Species
View(testing)
table(pred,testing$Species)
qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main="newdata Predictions")
qplot(Petal.Width,Petal.Length,colour=PredictRight,data=testing,main="newdata Predictions")
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
library(caret)
library(caret)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
modlda <- train(Species ~.,data=training,method="lda")
modnb <- train(Species ~.,data=training,method="nb")
equalPredictions <- (plda == pnb)
qplot(Petal.Width,Sepal.Width,colour=equalPredictions,data=testing)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
names(segmentationOrigin)
names(segmentationorigin)
names(segmentationOriginal)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p = 0.7, list = FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
dim(training); dim(testing)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
names(segmentationOriginal)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p = 0.7, list = FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
dim(training); dim(testing)
setseed(125)
set.seed(125)
model <- train(Class~.,method="rpart",data=training)
model$finalmodel
model$finalModel
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
plot(model$finalModel, uniform = TRUE, main = "Classification Tree")
text(model$finalModel, use.n = TRUE, all = TRUE, cex = .8)
text(model$finalModel, use.n = TRUE, all = TRUE, cex = .6)
plot(model$finalModel, uniform = TRUE, main = "Classification Tree")
text(model$finalModel, use.n = TRUE, all = TRUE, cex = .6)
library(pgmm)
data(olive)
olive = olive[,-1]
View(olive)
View(olive)
View(olive)
ModFit <- train(Area~.,method="rpart2",data=olive)
MOdFit$finalModel
ModFit$finalModel
plot(ModFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(ModFit$finalModel, use.n = TRUE, all = TRUE, cex = .6)
newdata = as.data.frame(t(colMeans(olive)))
View(newdata)
?predict
predict(ModFit,data=newdata)
predict(ModFit,newdata)
plot(ModFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(ModFit$finalModel, use.n = TRUE, all = TRUE, cex = .6)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
View(SAheart)
names(SAheart)
ModFit4 <-train(chd~age+alcohol+obesity+tobacco+typea+ldl,method="glm",family="binomial",data=trainSA)
missClass(trainSA$chd,predict(ModFit4,trainSA))
missClass(testSA$chd,predict(ModFit4,testSA))
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd,predict(ModFit4,trainSA))
missClass(testSA$chd,predict(ModFit4,testSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
View(vowel.test)
View(vowel.train)
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
library(caret)
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
library("randomForest", lib.loc="C:/Program Files/R/R-3.1.2/library")
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
plot(model$finalModel, uniform = TRUE, main = "Classification Tree")
text(model$finalModel, use.n = TRUE, all = TRUE, cex = .6)
missClass(trainSA$chd,predict(ModFit4,trainSA))
missClass(testSA$chd,predict(ModFit4,testSA))
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
ModFit5 <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(ModFit5), decreasing=T)
ModFit5 <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(ModFit5), decreasing=T)
set.seed(125)
model <- train(Class~.,method="rpart",data=training)
model$finalModel
plot(model$finalModel, uniform = TRUE, main = "Classification Tree")
text(model$finalModel, use.n = TRUE, all = TRUE, cex = .6)
plot(model$finalModel, uniform = TRUE, main = "Classification Tree")
text(model$finalModel, use.n = TRUE, all = TRUE, cex = .6)
library(ElemStatLearn); data(prostate)
str(prostate)
# regression subset selection in the prostate dataset
library(ElemStatLearn)
data(prostate)
covnames <- names(prostate[-(9:10)])
y <- prostate$lpsa
x <- prostate[,covnames]
form <- as.formula(paste("lpsa~", paste(covnames, collapse="+"), sep=""))
summary(lm(form, data=prostate[prostate$train,]))
set.seed(1)
train.ind <- sample(nrow(prostate), ceiling(nrow(prostate))/2)
y.test <- prostate$lpsa[-train.ind]
x.test <- x[-train.ind,]
y <- prostate$lpsa[train.ind]
x <- x[train.ind,]
p <- length(covnames)
rss <- list()
for (i in 1:p) {
cat(i)
Index <- combn(p,i)
rss[[i]] <- apply(Index, 2, function(is) {
form <- as.formula(paste("y~", paste(covnames[is], collapse="+"), sep=""))
isfit <- lm(form, data=x)
yhat <- predict(isfit)
train.rss <- sum((y - yhat)^2)
yhat <- predict(isfit, newdata=x.test)
test.rss <- sum((y.test - yhat)^2)
c(train.rss, test.rss)
})
}
png("Plots/selection-plots-01.png", height=432, width=432, pointsize=12)
plot(1:p, 1:p, type="n", ylim=range(unlist(rss)), xlim=c(0,p), xlab="number of predictors", ylab="residual sum of squares", main="Prostate cancer data")
for (i in 1:p) {
points(rep(i-0.15, ncol(rss[[i]])), rss[[i]][1, ], col="blue")
points(rep(i+0.15, ncol(rss[[i]])), rss[[i]][2, ], col="red")
}
minrss <- sapply(rss, function(x) min(x[1,]))
lines((1:p)-0.15, minrss, col="blue", lwd=1.7)
minrss <- sapply(rss, function(x) min(x[2,]))
lines((1:p)+0.15, minrss, col="red", lwd=1.7)
legend("topright", c("Train", "Test"), col=c("blue", "red"), pch=1)
dev.off()
##
# ridge regression on prostate dataset
library(MASS)
lambdas <- seq(0,50,len=10)
M <- length(lambdas)
train.rss <- rep(0,M)
test.rss <- rep(0,M)
betas <- matrix(0,ncol(x),M)
for(i in 1:M){
Formula <-as.formula(paste("y~",paste(covnames,collapse="+"),sep=""))
fit1 <- lm.ridge(Formula,data=x,lambda=lambdas[i])
betas[,i] <- fit1$coef
scaledX <- sweep(as.matrix(x),2,fit1$xm)
scaledX <- sweep(scaledX,2,fit1$scale,"/")
yhat <- scaledX%*%fit1$coef+fit1$ym
train.rss[i] <- sum((y - yhat)^2)
scaledX <- sweep(as.matrix(x.test),2,fit1$xm)
scaledX <- sweep(scaledX,2,fit1$scale,"/")
yhat <- scaledX%*%fit1$coef+fit1$ym
test.rss[i] <- sum((y.test - yhat)^2)
}
png(file="Plots/selection-plots-02.png", width=432, height=432, pointsize=12)
plot(lambdas,test.rss,type="l",col="red",lwd=2,ylab="RSS",ylim=range(train.rss,test.rss))
lines(lambdas,train.rss,col="blue",lwd=2,lty=2)
best.lambda <- lambdas[which.min(test.rss)]
abline(v=best.lambda+1/9)
legend(30,30,c("Train","Test"),col=c("blue","red"),lty=c(2,1))
dev.off()
png(file="Plots/selection-plots-03.png", width=432, height=432, pointsize=8)
plot(lambdas,betas[1,],ylim=range(betas),type="n",ylab="Coefficients")
for(i in 1:ncol(x))
lines(lambdas,betas[i,],type="b",lty=i,pch=as.character(i))
abline(h=0)
legend("topright",covnames,pch=as.character(1:8))
dev.off()
#######
# lasso
library(lars)
lasso.fit <- lars(as.matrix(x), y, type="lasso", trace=TRUE)
png(file="Plots/selection-plots-04.png", width=432, height=432, pointsize=8)
plot(lasso.fit, breaks=FALSE)
legend("topleft", covnames, pch=8, lty=1:length(covnames), col=1:length(covnames))
dev.off()
# this plots the cross validation curve
png(file="Plots/selection-plots-05.png", width=432, height=432, pointsize=12)
lasso.cv <- cv.lars(as.matrix(x), y, K=10, type="lasso", trace=TRUE)
dev.off()
# Prostate cancer
library(ElemStatLearn); data(prostate)
str(prostate)
small = prostate[1:5,]
View(small)
lm(lpsa ~ .,data =small)
library(ISLR); data(Wage); library(ggplot2); library(caret);
head(wage)
head(Wage)
Wage <- subset(Wage,select=-c(logwage))
head(Wage)
inBuild <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
validation <- Wage[-inBuild,]
buildData <- Wage[inBuild,]
inTrain <- createDataPartition(y=buildData$wage,
p=0.7, list=FALSE)
training <- buildData[inTrain,]; testing <- buildData[-inTrain,]
dim(training)
dim(testing)
dim(validation)
mod1 <- train(wage ~.,method="glm",data=training)
mod2 <- train(wage ~.,method="rf",
data=training,
trControl = trainControl(method="cv"),number=3)
pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)
qplot(pred1,pred2,colour = wage,data = testing)
preDF <- data.frame(pred1,pred2,wage=testing$wage)
pred1
View(preDF)
combModFit <- train(wage ~.,method="gam",data=predDF)
combModFit <- train(wage ~.,method="gam",data=preDF)
?train
combPred <- predict(combModFit,preDF)
combPred
sqrt(sum((pred1-testing$wage)^2))
sqrt(sum((pred2-testing$wage)^2))
sqrt(sum((comPred-preDF$wage)^2))
sqrt(sum((combPred-preDF$wage)^2))
pred1V <- predict(mod1,validation)
pred2V <- predict(mod2,validation)
predVDF <- data.frame(pred1=pred1V,pred2=pred2V)
View(predVDF)
combPredV <- predict(combModFit,predVDF)
sqrt(sum((pred1V-validation$wage)^2))
sqrt(sum((pred2V-validation$wage)^2))
sqrt(sum((combPredV-validation$wage)^2))
setwd("D:/JOHNS HOPKINS Data Science Program/Developing Data Product/rmb/SimplestAPP")
runApp()
library(shiny)
runApp()
shinyUI(pageWithSidebar(
headerPanel("Illustrating markup"),
sidebarPanel(
h1('Sidebar panel'),
h1('H1 text'),
h2('H2 Text'),
h3('H3 Text'),
h4('H4 Text')
),
mainPanel(
h3('Main Panel text'),
code('some code'),
p('some ordinary text')
)
))
runApp()
setwd("D:/JOHNS HOPKINS Data Science Program/Developing Data Product/rmb/htmlMarkupAPP")
runApp()
runApp()
shinyUI(pageWithSidebar(
headerPanel("Illustrating inputs"),
sidebarPanel(
numericInput('id1', 'Numeric input, labeled id1', 0, min = 0, max = 10, step = 1),
checkboxGroupInput("id2", "Checkbox",
c("Value 1" = "1",
"Value 2" = "2",
"Value 3" = "3")),
dateInput("date", "Date:")
),
mainPanel(
h3('Illustrating outputs'),
h4('You entered'),
verbatimTextOutput("oid1"),
h4('You entered'),
verbatimTextOutput("oid2"),
h4('You entered'),
verbatimTextOutput("odate")
)
))
runApp()
runApp()
setwd("D:/JOHNS HOPKINS Data Science Program/Developing Data Product/rmb/illustraingAPP")
runApp()
library(shiny)
runApp()
runApp()
